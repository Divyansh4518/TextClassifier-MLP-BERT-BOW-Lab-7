# Text Classification using MLP (Bag-of-Words & Embeddings)

This repository contains an implementation of a **text classification model** using a **Multi-Layer Perceptron (MLP)**. Two feature extraction techniques are used in parallel:
- **Bag-of-Words (BoW)** with `CountVectorizer`
- **Pretrained Embeddings** from `bert-base-uncased`

The project follows a **checkpoint-based training** approach and supports continual learning with the **IMDB dataset**.

---
## ğŸ“Œ **Project Overview**

### **1. Dataset Preparation**
- Splits the dataset into **train (80%)**, **validation (20%)**, and **test** sets.
- Uses **Bag-of-Words** and **BERT embeddings** as parallel text representations.

### **2. MLP Model Architecture**
- Input Layer (Depends on feature size)
- Hidden Layers: `[512 â†’ 256 â†’ 128 â†’ 64]` with ReLU activations
- Output Layer: **2 neurons (binary classification)**

### **3. Training Process**
- **BoW and Embeddings models train separately** using the same MLP architecture.
- Uses **Adam optimizer** (`lr=0.001`) and **CrossEntropyLoss**.
- Implements **checkpointing** to save the best model (`checkpoint_bow.pt` & `checkpoint_embed.pt`).
- **TensorBoard Logging** for loss and accuracy tracking.

### **4. Continual Learning on IMDB Dataset**
- Loads the **best-performing checkpoint** and resumes training.
- Fine-tunes on **IMDB dataset** with `lr=0.0001`.

---
## ğŸš€ **Setup Instructions**

### **1. Clone the Repository**
```sh
https://github.com/Divyansh4518/TextClassifier-MLP-BERT-BOW-Lab-7.git
```

### **2. Install Dependencies**
```sh
pip install -r requirements.txt
```

### **3. Run Training**
```sh
python train.py
```

### **4. Run Evaluation**
```sh
python evaluate.py
```

### **5. Visualize Logs in TensorBoard**
```sh
tensorboard --logdir=runs
```
Then, open [http://localhost:6006/](http://localhost:6006/) in your browser.

---
## ğŸ“Š **Results & Metrics**
- **Final Accuracy** on Dataset 1 and IMDB
- **Confusion Matrix** visualization
- **Training & Validation Loss Curves**

---
## ğŸ“ **Project Structure**
```
ğŸ“‚ MLP-Text-Classification
â”œâ”€â”€ ğŸ“œ README.md
â”œâ”€â”€ ğŸ“œ requirements.txt
â”œâ”€â”€ ğŸ“œ train.py
â”œâ”€â”€ ğŸ“œ evaluate.py
â”œâ”€â”€ ğŸ“œ dataset_loader.py
â”œâ”€â”€ ğŸ“œ model.py
â”œâ”€â”€ ğŸ“œ utils.py
â”œâ”€â”€ ğŸ“‚ runs/  # TensorBoard logs
â”œâ”€â”€ ğŸ“‚ checkpoints/  # Model checkpoints
â””â”€â”€ ğŸ“‚ data/  # Training & testing datasets
```

---
## ğŸ¤– **Future Improvements**
- Implement **Transformer-based classifiers** (e.g., `DistilBERT`, `RoBERTa`).
- Experiment with **Llama-3.1-8B** for embeddings.
- Support **multi-class text classification**.

---
## ğŸ“œ **License**
This project is licensed under the **MIT License**.

